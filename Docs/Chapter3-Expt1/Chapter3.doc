Chapter 3 – Framework of the Experiments

What is the methods
	Here we describe the algorithm, first patching the data, 
	the random selection of patches, finding clusters of the patches
	Mapping clusters to GT
	For new sample → Nearest Cluster, replace by GT cluster
	Repatch to get the image

Define the algorithm
How the data is generated
Show the clusters formed
	Clusters
Preprocessing Steps
Results

1) No of clusters
2) Sample Size
3) ??
The chapter aims to present our clustering based image segmentation methods.
The chapter is divided into 4 parts. In the first part we discuss the basic framework of our algorithm. Part 2 and Part3 talk about the various preprocessing methods and how the methods was applied to our problem.


2.1 Image segmentation Framework

Our image segmentation framework takes its input a list of fundus images I = (I_1, I_2.... I_n) and their correspoding ground truth segmentation maps S = (S_1, S_2.....S_n). The given fundus images can be grayscale images or RGB color images. Each of the given segmentaion maps in S, is the same size as that of correspoding fundus image in I. The ground truth segmentation is a binary image I.e, each pixel in the segmentation map is either 1 or a 0, where 1 depeicts the presenece of a vessel and 0 for the background.

So given a training dataset consisting of fundus images I and segmentation maps S, the aim is to define a framework, which can learn to predict the segmentation map S' for an unknown new image I'.

The proposed problem is dealt in a patch based framework. Each image pixel in the image is represented by a patch centered around the pixel. The image is decomposed into such dense patches.

Given an Image pixel I(x,y) we compute a patch of size k x k around the pixel I(x,y). Each image is then described by such patches.

Patches are computed both for the Fundus images and their corresoponding binary segmentation map images. For an Image I, and its segmentation map S, the patches our denoted by 
I ~ [Ip_1,Ip_2 …]  and S ~ [Sp_1,Sp_2,....]

The aim of our thesis is to develop algorithms which predict the Segmentation patch for a given image patch.

During the training process, the algorithms aim to learn a set of image representation patches describing the patches from the training set.

Give a training set consisting of image patches X={X1, X2, X3. … } and their corresponding ground truth patches Y = {Y1,Y2,Y3,...}, we aim to learn as set Xr = {Xr1,Xr2,.... } which can represent each and every patch of in the training images X.

In the prediction phase, a single channel of the image is taken and dense patches (patches per pixel are calculated ) and fed to the classifier. The classifer predicts the GT patches for each image patch. The ground truth patches are the recombined to obtain the segmentation map for the image.

Preprocessing
In our task we have explored some of the preprocessing steps including Patch Normalization, Contrast streteching, Local contrast nomralization.

All the input patches are normalizaed by subtracting the mean and standard deviation per patch. In addition to this we also test with normalizing the entire image before patch generation.

In some of the experiments, we improve the image contrast by utilizing Contrast Limited Adaptive Histogram Equalization (CLAHE) as explained in section{}.

Rotations
To provide for rotational invariance and learn better representatives, rotated patches are also included in our learning phase. Each of the training images are additionaly rotated at angles of 30,60,90 and patches are calculated.


Datasets
For testing the performance of our algorithm , we train and test our system on the following publically available datasets. In this section we describe the characteristics of these datsets

DRIVE Dataset
The Digital Retinal Images for Vessel Extraction (DRIVE) dataset consists of 40 color retinal images randomly selected from diabetic retinopathy screening program for 400 diabetic patients.Each of these images in dataset are JPEG compressed and have a dimensions of 768 x 584 pixels captured at a resolution on bits per pixel. The images are captures with a 45degree field of view (FOV).
Of the 40 images in the datset, 7 show sign of diabetic retinopathy , while the remaining 33 do not consist of any pathology. Each image is provided with a corresponding mask delineating the FOV.

The dataset is divided is provided with divisions in terms of training and testing set, with each set consisting of 20 images. Each of the 40 images have been manually segmented by human observers trained by an experienced optamologist. For the training set, single ground truth segmentation of the vessels is provided. The test set is provided with two ground truth segmentations, of which the first one is used as gold standard and the other is used to compare the performance with an independent human observer.

STARE Dataset
The STARE dataset consists of 20 images with blood vessel segmentations, out of which 10 show signs of pathology. The images have been capture with a FOV of 35degrees at 8 bit per pixel resolution, with dimesnsions of each image as 605 x 700 pixels. The dataset consists of segmentation provided by two human observers. In our experiements, we consider the segmentations provided by the first observer as ground truth.

For the experiements, the dataset is randomly divided into training and test sets each consisting of 10images.

HRF dataset
The High Resolution Fundus (HRF) image databse consists of 45 images of which 15 images come from healthy patients , 15 from patients with diabetic retinopathy and 15 of glacumatous patients.

The images were captured with a FOV of 60degrees, at a high resolution of 24bits per pixel. The size of each image is 3504 x 2336 pixels and stored with JPEG compression. 

Each image is provided a manual segmentation of vessesl as segmented by three independent human observers trained by experienced optahmologists.The dataset also provides a corresponging mask image of each image delineating the FOV.

ARIA Dataset
The ARIA dataset consists of three groups of images. One of the group consists of 92 images with age-related macular degeneration, the other with 59 images from diabetic patients and the last group with 61 images from a control group.
The images are captures with a 50degree FOV, stored in uncompressed TIFF format, with a resolution of 8bits oer pixel. Each image has dimensions of 768 x 576 pixels.

The dataset provides with blood vessel segmentation images as manually segmented by experts and a corresponding mask delinating the FOV region.



In the next 2 chapters, we discuss our two models used to learn the image representatives.


